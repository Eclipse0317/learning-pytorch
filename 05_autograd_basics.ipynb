{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13348079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea1193",
   "metadata": {},
   "source": [
    "# Tracking Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aedc0025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a: tensor([2., 3.], requires_grad=True)\n",
      "Tensor b: tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors and set requires_grad=True to track computation\n",
    "# By default, requires_grad is False\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0abff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an operation involving these tensors\n",
    "# Let Q = 3a^3 - b^2\n",
    "# PyTorch builds a computation graph behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f30fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term 1: 3a^3\n",
    "a_cubed = a.pow(3)\n",
    "term1 = 3 * a_cubed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659b00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term 2: b^2\n",
    "term2 = b.pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ed3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = 3a^3 - b^2:\n",
      "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Final Q\n",
    "Q = term1 - term2\n",
    "print(f\"Q = 3a^3 - b^2:\\n{Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb8d8f",
   "metadata": {},
   "source": [
    "# Computing Gradients with backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552fc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q is not a scalar. Usually, .backward() is called on a scalar loss value.\n",
    "# To illustrate, let's calculate a single scalar value from Q, e.g., the mean.\n",
    "# This external_grad is implicitly torch.tensor(1.0) when backward() is called on a scalar.\n",
    "external_grad = torch.tensor([1.0, 1.0]) # Gradient for Q itself (needed as Q is not scalar)\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb541ac",
   "metadata": {},
   "source": [
    "# Check Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5228ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients are now populated in the .grad attribute of the original tensors (a and b)\n",
    "# It contains dQ/da and dQ/db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75989f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for a (dQ/da = 9a^2): tensor([36., 81.])\n"
     ]
    }
   ],
   "source": [
    "# Check gradients dQ/da = 9a^2\n",
    "print(f\"Gradients for a (dQ/da = 9a^2): {a.grad}\") # Should be 9 * [2^2, 3^2] = 9 * [4, 9] = [36, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d502bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for b (dQ/db = -2b): tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "# Check gradients dQ/db = -2b\n",
    "print(f\"Gradients for b (dQ/db = -2b): {b.grad}\") # Should be -2 * [6, 4] = [-12, -8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d40f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Important Notes about .backward() ---\n",
    "# 1. Gradients are accumulated: If you call backward() multiple times, gradients add up.\n",
    "#    You usually need to zero out gradients before each training iteration using optimizer.zero_grad().\n",
    "# 2. Only leaf nodes get gradients: By default, only tensors created directly by the user\n",
    "#    with requires_grad=True will have their .grad populated. Intermediate tensors (like 'term1') won't.\n",
    "# 3. backward() on non-scalar output: If you call backward() on a tensor with more than one element (like Q),\n",
    "#    you need to provide a 'gradient' argument of the same shape, representing the gradient\n",
    "#    of the final scalar loss with respect to that tensor (often just torch.ones_like(Q) or as shown above).\n",
    "#    If the output *is* scalar (like Q.mean()), you can just call .backward() without arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6a383",
   "metadata": {},
   "source": [
    "# Example with scalar output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8b6c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 2.0, y: 3.0\n",
      "z = x**2 * y + x*y: 18.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "z = x**2 * y + x*y\n",
    "print(f\"x: {x}, y: {y}\")\n",
    "print(f\"z = x**2 * y + x*y: {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b04364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradients dz/dx and dz/dy\n",
    "z.backward() # No gradient argument needed as z is scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bade91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient dz/dx: 15.0\n",
      "Gradient dz/dy: 6.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient dz/dx: {x.grad}\") # dz/dx = 2xy + y = 2*2*3 + 3 = 15\n",
    "print(f\"Gradient dz/dy: {y.grad}\") # dz/dy = x^2 + x = 2^2 + 2 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ab7215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients after zeroing: x.grad=0.0, y.grad=0.0\n"
     ]
    }
   ],
   "source": [
    "# Zero the gradients before potential next calculation\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "print(f\"\\nGradients after zeroing: x.grad={x.grad}, y.grad={y.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf332722",
   "metadata": {},
   "source": [
    "# Excluding Blocks from Tracking: torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc534ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q requires grad: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Q requires grad: {Q.requires_grad}\") # Q requires grad because it depends on a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f422b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_no_grad requires grad: False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Operations inside this block are NOT tracked\n",
    "    Q_no_grad = 3*a**3 - b**2\n",
    "    print(f\"Q_no_grad requires grad: {Q_no_grad.requires_grad}\") # This will be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf8f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_again requires grad: True\n"
     ]
    }
   ],
   "source": [
    "# Operations outside the block are tracked again if inputs require grad\n",
    "Q_again = 3*a**3 - b**2\n",
    "print(f\"Q_again requires grad: {Q_again.requires_grad}\") # This will be True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
