{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0fbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Neural network modules\n",
    "import torch.optim as optim # Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71dc98",
   "metadata": {},
   "source": [
    "# 1. Defining a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6e3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models in PyTorch are typically classes inheriting from nn.Module\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleLinearModel, self).__init__() # Call parent class constructor\n",
    "        # Define the layers the model will use\n",
    "        # nn.Linear implements a standard linear transformation: output = input @ weight.T + bias\n",
    "        self.linear_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the forward pass: how input 'x' flows through the defined layers\n",
    "        # In this case, input x just goes through the linear layer\n",
    "        out = self.linear_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2da9c",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8e1205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLinearModel(\n",
      "  (linear_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_features = 10 # Example: input data has 10 features\n",
    "output_classes = 1 # Example: output is a single value (e.g., for regression)\n",
    "\n",
    "model = SimpleLinearModel(input_features, output_classes)\n",
    "print(model) # Prints the layers in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadd650",
   "metadata": {},
   "source": [
    "## Inspect Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247d6af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: linear_layer.weight\n",
      "Shape: torch.Size([1, 10])\n",
      "Values: tensor([[ 0.0329, -0.1560, -0.2677, -0.1479,  0.0884, -0.2830, -0.2560, -0.2099,\n",
      "          0.1561, -0.3042]])\n",
      "Parameter: linear_layer.bias\n",
      "Shape: torch.Size([1])\n",
      "Values: tensor([-0.1320])\n"
     ]
    }
   ],
   "source": [
    "# nn.Module automatically tracks parameters of its defined layers\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Values: {param.data}\")\n",
    "\n",
    "# Note: These parameters are initlalized randomly.\n",
    "# The goal of training is to adjust these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71e44a",
   "metadata": {},
   "source": [
    "# 2. Defining a Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43514b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n"
     ]
    }
   ],
   "source": [
    "# Loss functions measure the difference between model output and target values.\n",
    "# Let's choose Mean Squared Error (MSE) loss, common for regression tasks.\n",
    "loss_function = nn.MSELoss()\n",
    "print(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c11cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some dummy model output and target values\n",
    "# Assume a batch size of 4 samples\n",
    "dummy_output = torch.randn(4, output_classes) # Model predicts 1 value for each of 4 samples\n",
    "dummy_target = torch.randn(4, output_classes) # True values for the 4 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874b10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Calculation:\n",
      "Dummy Model Output:\n",
      "tensor([[ 1.6352],\n",
      "        [-1.7905],\n",
      "        [ 0.5569],\n",
      "        [ 0.9428]])\n",
      "Dummy Target Values:\n",
      "tensor([[-0.1020],\n",
      "        [-0.2389],\n",
      "        [-0.3286],\n",
      "        [-0.5212]])\n",
      "Calculated MSE Loss: 2.0881645679473877\n"
     ]
    }
   ],
   "source": [
    "loss = loss_function(dummy_output, dummy_target)\n",
    "print(\"Example Calculation:\")\n",
    "print(f\"Dummy Model Output:\\n{dummy_output}\")\n",
    "print(f\"Dummy Target Values:\\n{dummy_target}\")\n",
    "print(f\"Calculated MSE Loss: {loss.item()}\") # .item() gets the scalar value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198dbf8",
   "metadata": {},
   "source": [
    "# 3. Defining an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e046bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimizer updates the model's parameters using the gradients computed by autograd.\n",
    "# We need to tell it WHICH parameters to optimize and set a learning rate.\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541449fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (SGD) is a common basic optimizer.\n",
    "# We pass model.parameters() so the optimizer knows what tensors to update.\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233554d",
   "metadata": {},
   "source": [
    "## How these pieces will interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f29db6",
   "metadata": {},
   "source": [
    "1. Get input data, pass through model: output = model(input_data)\n",
    "2. Calculate loss: loss = loss_function(output, target_data)\n",
    "3. Calculate gradients: loss.backward()  <-- Uses autograd!\n",
    "4. Update parameters: optimizer.step()   <-- Uses gradients stored in param.grad\n",
    "5. Zero gradients for next iteration: optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d517369",
   "metadata": {},
   "source": [
    "# 4. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e22f175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some data for the model to learn from.\n",
    "# Let's create synthetic data where Y is linearly related to X plus some noise.\n",
    "# Target relationship: Y = 2*X + 1 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c525d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need gradients for the training data itself\n",
    "X_train = torch.randn(100, input_features) * 5 # 100 samples, 10 features\n",
    "true_weights = torch.tensor([[2.0] * input_features])\n",
    "true_bias = torch.tensor([[1.0]])\n",
    "Y_train = X_train @ true_weights.T + true_bias + torch.randn(100, output_classes) # Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a39eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([100, 10])\n",
      "Shape of Y_train: torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of Y_train: {Y_train.shape}\")\n",
    "# Note: Our model starts with random weights/bias. Training aims to make them close to [2, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e32c4",
   "metadata": {},
   "source": [
    "# 5. The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "023b9d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.7432\n",
      "Epoch [20/100], Loss: 1.1675\n",
      "Epoch [30/100], Loss: 1.0902\n",
      "Epoch [40/100], Loss: 1.0368\n",
      "Epoch [50/100], Loss: 0.9995\n",
      "Epoch [60/100], Loss: 0.9733\n",
      "Epoch [70/100], Loss: 0.9550\n",
      "Epoch [80/100], Loss: 0.9421\n",
      "Epoch [90/100], Loss: 0.9331\n",
      "Epoch [100/100], Loss: 0.9268\n"
     ]
    }
   ],
   "source": [
    "# This is where the learning happens!\n",
    "num_epochs = 100 # Number of times we iterate through the entire dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # === Core Training Loop ===\n",
    "\n",
    "    # 1. Forward Pass: Pass the data through the model\n",
    "    # We get the model's current predictions for X_train\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # 2. Calculate Loss: Compare model outputs to true values\n",
    "    loss = loss_function(outputs, Y_train)\n",
    "\n",
    "    # 3. Backward Pass: Calculate gradients of the loss w.r.t. model parameters.\n",
    "    # This uses the autograd engine we learned about.\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Optimizer Step: Update model parameters\n",
    "    # The optimizer uses the gradients computed in backward() and the learning rate\n",
    "    optimizer.step()\n",
    "\n",
    "    # 5. Zero Gradients: Clear gradients from the previous iteration.\n",
    "    # If we don't zero, gradients accumulate across iterations.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # === End Core Training Steps ===\n",
    "\n",
    "    # Print progress periodically to see if loss is decreasing\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e67221",
   "metadata": {},
   "source": [
    "# 6. Check Results (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec0a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters learned by the model:\n",
      "linear_layer.weight: [[1.9923012 1.9886528 2.031799  1.9995862 1.9901197 2.0149977 1.9897474\n",
      "  1.985668  2.0027409 1.9881456]]\n",
      "linear_layer.bias: [0.74791694]\n",
      "(Target parameters were weights: [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]], bias: [[1.]])\n"
     ]
    }
   ],
   "source": [
    "# Let's see how well the model learned the parameters.\n",
    "print(\"Parameters learned by the model:\")\n",
    "# Turn off gradient tracking for inspection\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: {param.data.numpy()}\")\n",
    "    print(f\"(Target parameters were weights: {true_weights.numpy()}, bias: {true_bias.numpy()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
